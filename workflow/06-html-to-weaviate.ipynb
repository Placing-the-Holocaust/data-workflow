{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import yaml\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import weaviate\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/sentence_transformers/models/Dense.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/05_final/*.html\")\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_testimony(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    metadata_include = [\"rg_number\", \"interviewee\", \"gender\", \"birth_year\", \"experience_group\", \"birth_country\"]\n",
    "\n",
    "    def get_metadata(soup):\n",
    "        # Extract the YAML front matter from the HTML\n",
    "        front_matter_text = soup.text.split('---\\n', 2)[1]\n",
    "        metadata = yaml.safe_load('---\\n' + front_matter_text)\n",
    "\n",
    "        # Only keep metadata found in metadata_include\n",
    "        filtered_metadata = {k: metadata[k] for k in metadata_include if k in metadata}\n",
    "        filtered_metadata[\"birth_country\"] = metadata.get(\"country\", \"\")\n",
    "        return filtered_metadata\n",
    "\n",
    "    def extract_windows(dialogue_tag, metadata):\n",
    "        p_tag = dialogue_tag.find('p')\n",
    "        sentences = p_tag.find_all(\"sentence\")\n",
    "        p_text = \" \".join([sent.text for sent in sentences])\n",
    "        windows = []\n",
    "        window_size = 3\n",
    "        sentence_ids = [sent['id'] for sent in sentences]\n",
    "        \n",
    "        if len(sentences) < window_size:\n",
    "            window_texts = \" \".join([sent.text for sent in sentences])\n",
    "            window = {\n",
    "                'sentence_ids': sentence_ids,\n",
    "                'text': window_texts,\n",
    "                'labels': count_labels(sentences)\n",
    "            }\n",
    "            window[\"category\"] = \"question\" if dialogue_tag.get('class') == ['Question'] else \"answer\"\n",
    "            window.update(metadata)\n",
    "            windows.append(window)\n",
    "        else:\n",
    "            for i in range(len(sentences) - window_size + 1):\n",
    "                window_sentences = sentences[i:i+window_size]\n",
    "                window_texts = \" \".join([sent.text for sent in window_sentences])\n",
    "                window = {\n",
    "                    'sentence_ids': sentence_ids[i:i+window_size],\n",
    "                    'text': window_texts,\n",
    "                    'labels': count_labels(window_sentences)\n",
    "                }\n",
    "                window[\"category\"] = \"question\" if dialogue_tag.get('class') == ['Question'] else \"answer\"\n",
    "                window.update(metadata)\n",
    "                windows.append(window)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def count_labels(sentences):\n",
    "        label_counters = {\n",
    "            'populated_place': 0, 'building': 0, 'country': 0, 'spatial_obj': 0, \n",
    "            'dlf': 0, 'int_space': 0, 'env_features': 0, 'region': 0, \n",
    "            'npip': 0, \"country\": 0,\n",
    "        }\n",
    "        for sentence in sentences:\n",
    "            for label in label_counters:\n",
    "                label_counters[label] += len(sentence.find_all(\"span\", {\"class\": label.lower()}))\n",
    "        \n",
    "        return label_counters\n",
    "\n",
    "    metadata = get_metadata(soup)\n",
    "    all_windows = []\n",
    "    for dialogue_tag in soup.find_all('dialogue'):\n",
    "        windows = extract_windows(dialogue_tag, metadata)\n",
    "        all_windows.extend(windows)\n",
    "\n",
    "    sentence_embeddings = model.encode([text[\"text\"] for text in all_windows])\n",
    "\n",
    "    combined_data = []\n",
    "\n",
    "    for i, window in enumerate(all_windows):\n",
    "        combined_dict = {\n",
    "            \"sentence_ids\": window['sentence_ids'],\n",
    "            \"text\": window['text'],\n",
    "            \"embedding\": sentence_embeddings[i],\n",
    "            \"category\": window[\"category\"]\n",
    "        }\n",
    "        combined_dict.update(window['labels'])\n",
    "        \n",
    "        label_map = {\"rg_number\": \"rg\", \"interviewee\": \"full_name\"}\n",
    "        for label in metadata_include:\n",
    "            new_label = label_map.get(label, label)\n",
    "            combined_dict[new_label] = window[label]\n",
    "        combined_data.append(combined_dict)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 979/979 [1:16:39<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "for file in tqdm(files):\n",
    "    output_file = file.split(\"/\")[-1].replace(\"_cleaned.html\", \".parquet\")\n",
    "    output_path = f\"../data/06_parquet/{output_file}\"\n",
    "    \n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        continue  # Skip this file and move to the next one\n",
    "    \n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "        data = data.replace(\"|\", \"I\")\n",
    "        result = process_testimony(data)\n",
    "        df = pd.DataFrame(result)\n",
    "        df['birth_year'] = df['birth_year'].replace('none', pd.NA)\n",
    "        df.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'HolocaustTestimonies' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.auth import Auth\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "WEAVIATE_URL = \"https://1tnwq0iequmnhcrcultoq.c0.us-east1.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY = \"pV6haMawVUuFMWiu8RGRkbrpjo4YfGehXHmV\"\n",
    "collection_name = \"HolocaustTestimonies\"\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "if client.collections.exists(collection_name):\n",
    "    client.collections.delete(collection_name)\n",
    "\n",
    "holocaust_testimonies = client.collections.create(\n",
    "    name=collection_name,\n",
    "    properties=[\n",
    "        wvc.config.Property(\n",
    "            name=\"sentence_ids\",\n",
    "            data_type=wvc.config.DataType.TEXT_ARRAY\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"text\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"category\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"populated_place\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"building\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"country\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"spatial_obj\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"dlf\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"int_space\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"env_features\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"region\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"npip\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"experience_group\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"birth_country\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"gender\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"rg\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"full_name\",\n",
    "            data_type=wvc.config.DataType.TEXT\n",
    "        ),\n",
    "        wvc.config.Property(\n",
    "            name=\"birth_year\",\n",
    "            data_type=wvc.config.DataType.INT\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(client.is_live())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 979 parquet files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data:   0%|          | 0/1050 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data: 100%|██████████| 1050/1050 [00:00<00:00, 30601.32it/s]\n",
      "Inserting data: 100%|██████████| 1050/1050 [00:00<00:00, 8973.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion complete.\n"
     ]
    }
   ],
   "source": [
    "# Get list of parquet files\n",
    "parquet_files = glob.glob(\"../data/06_parquet/*.parquet\")\n",
    "parquet_files.sort()\n",
    "print(f\"Found {len(parquet_files)} parquet files.\")\n",
    "\n",
    "# Function to prepare a single row\n",
    "def prepare_row(row):\n",
    "    return {\n",
    "        \"properties\": {\n",
    "            \"sentence_ids\": row['sentence_ids'],\n",
    "            \"text\": row['text'],\n",
    "            \"category\": row['category'],\n",
    "            \"populated_place\": int(row['populated_place']),\n",
    "            \"building\": int(row['building']),\n",
    "            \"country\": int(row['country']),\n",
    "            \"spatial_obj\": int(row['spatial_obj']),\n",
    "            \"dlf\": int(row['dlf']),\n",
    "            \"int_space\": int(row['int_space']),\n",
    "            \"env_features\": int(row['env_features']),\n",
    "            \"region\": int(row['region']),\n",
    "            \"npip\": int(row['npip']),\n",
    "            \"experience_group\": row['experience_group'],\n",
    "            \"birth_country\": row['birth_country'],\n",
    "            \"gender\": row['gender'],\n",
    "            \"rg\": row['rg'],\n",
    "            \"full_name\": row['full_name'],\n",
    "            \"birth_year\": int(row['birth_year'])\n",
    "        },\n",
    "        \"vector\": row['embedding']\n",
    "    }\n",
    "\n",
    "# Process each parquet file\n",
    "for parquet_file in parquet_files[:1]:\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Prepare all the data rows\n",
    "    data_rows = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing data\"):\n",
    "        data_rows.append(prepare_row(row))\n",
    "    \n",
    "    # Perform batch insertion\n",
    "    with holocaust_testimonies.batch.fixed_size(batch_size=100) as batch:\n",
    "        for data_row in tqdm(data_rows, desc=\"Inserting data\"):\n",
    "            batch.add_object(\n",
    "                properties=data_row['properties'],\n",
    "                vector=data_row['vector']\n",
    "            )\n",
    "\n",
    "print(\"Data insertion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(data_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "holocaust_testimonies = client.collections.get(\"HolocaustTestimonies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(query, threshold):\n",
    "    query_vector = model.encode([query])[0]\n",
    "    response = holocaust_testimonies.query.near_vector(\n",
    "        near_vector=query_vector,\n",
    "        limit=10,\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "    for o in response.objects:\n",
    "        if o.metadata.distance < threshold:\n",
    "            print(o.properties[\"book\"], o.properties[\"chapter\"], o.properties[\"verse\"], \": \", o.properties[\"text\"])\n",
    "            print(o.metadata.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryReturn(objects=[])\n"
     ]
    }
   ],
   "source": [
    "query = \"We were not ther\"\n",
    "find_similar(query, threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasthtml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
