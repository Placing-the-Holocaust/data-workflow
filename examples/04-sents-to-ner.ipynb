{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RG Number</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>USHMM URL</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Middle Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Birth Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>...</th>\n",
       "      <th>Ghetto</th>\n",
       "      <th>Camp(s) Encyclopedia</th>\n",
       "      <th>Camp</th>\n",
       "      <th>Non-SS Camp</th>\n",
       "      <th>Region</th>\n",
       "      <th>Needs Research</th>\n",
       "      <th>Data Entry</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Notes:</th>\n",
       "      <th>Revisit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RG-50.549.02.0033</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Hetty</td>\n",
       "      <td>d'Ancona de</td>\n",
       "      <td>Leeuwe</td>\n",
       "      <td>Hetty D'Ancona</td>\n",
       "      <td>F</td>\n",
       "      <td>1930-05-01</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1999.A.0293</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RG-50.549.02.0072</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Emanuel</td>\n",
       "      <td>None</td>\n",
       "      <td>Mandel</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>GG</td>\n",
       "      <td>2003.205</td>\n",
       "      <td>Follow-up interview</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RG-50.549.02.0035</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Judith</td>\n",
       "      <td>None</td>\n",
       "      <td>Meisel</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Kaunas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>GG</td>\n",
       "      <td>1999.A.0024</td>\n",
       "      <td>This is a follow-up interview to one already d...</td>\n",
       "      <td>checked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RG-50.471.0015</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Esther</td>\n",
       "      <td>None</td>\n",
       "      <td>Lurie</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1998.A.0119.15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RG-50.030.0585</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Eugene</td>\n",
       "      <td>None</td>\n",
       "      <td>Miller</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1923-10-16</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lodz</td>\n",
       "      <td>Auschwitz,Dachau</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>GG</td>\n",
       "      <td>2010.249</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>RG-50.549.02.0073</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Flory</td>\n",
       "      <td>None</td>\n",
       "      <td>Jagoda</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>1923-12-21</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GG</td>\n",
       "      <td>2004.48</td>\n",
       "      <td>Follow-up</td>\n",
       "      <td>checked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>RG-50.030.0137</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Cornelius</td>\n",
       "      <td>None</td>\n",
       "      <td>Loen</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1922-05-02</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1990.437.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>RG-50.030.0058</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Isaac</td>\n",
       "      <td>None</td>\n",
       "      <td>Danon</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GG</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>RG-50.549.02.0078</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Lucie</td>\n",
       "      <td>None</td>\n",
       "      <td>Rosenberg</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>CL</td>\n",
       "      <td>2004.214</td>\n",
       "      <td>Not a survivor, volunteered for the museum?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>RG-50.549.02.0038</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Isaac</td>\n",
       "      <td>None</td>\n",
       "      <td>Danon</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1929-06-24</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1999.A.0038</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>977 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             RG Number                                            PDF URL  \\\n",
       "0    RG-50.549.02.0033  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "1    RG-50.549.02.0072  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "2    RG-50.549.02.0035  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "3       RG-50.471.0015  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "4       RG-50.030.0585  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "..                 ...                                                ...   \n",
       "972  RG-50.549.02.0073  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "973     RG-50.030.0137  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "974     RG-50.030.0058  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "975  RG-50.549.02.0078  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "976  RG-50.549.02.0038  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "\n",
       "                                             USHMM URL First Name  \\\n",
       "0    https://collections.ushmm.org/search/catalog/i...      Hetty   \n",
       "1    https://collections.ushmm.org/search/catalog/i...    Emanuel   \n",
       "2    https://collections.ushmm.org/search/catalog/i...     Judith   \n",
       "3    https://collections.ushmm.org/search/catalog/i...     Esther   \n",
       "4    https://collections.ushmm.org/search/catalog/i...     Eugene   \n",
       "..                                                 ...        ...   \n",
       "972  https://collections.ushmm.org/search/catalog/i...      Flory   \n",
       "973  https://collections.ushmm.org/search/catalog/i...  Cornelius   \n",
       "974  https://collections.ushmm.org/search/catalog/i...      Isaac   \n",
       "975  https://collections.ushmm.org/search/catalog/i...      Lucie   \n",
       "976  https://collections.ushmm.org/search/catalog/i...      Isaac   \n",
       "\n",
       "     Middle Name  Last Name      Birth Name Gender  Birth Date  Birth Year  \\\n",
       "0    d'Ancona de     Leeuwe  Hetty D'Ancona      F  1930-05-01      1930.0   \n",
       "1           None     Mandel            None      M        None      1936.0   \n",
       "2           None     Meisel            None      F        None      1929.0   \n",
       "3           None      Lurie            None      F        None         NaN   \n",
       "4           None     Miller            None      M  1923-10-16      1923.0   \n",
       "..           ...        ...             ...    ...         ...         ...   \n",
       "972         None     Jagoda            None      F  1923-12-21      1923.0   \n",
       "973         None       Loen            None      M  1922-05-02      1922.0   \n",
       "974         None      Danon            None      M        None      1929.0   \n",
       "975         None  Rosenberg            None      F        None      1921.0   \n",
       "976         None      Danon            None      M  1929-06-24      1929.0   \n",
       "\n",
       "     ...  Ghetto Camp(s) Encyclopedia  Camp Non-SS Camp   Region  \\\n",
       "0    ...    None                 None  None          None   None   \n",
       "1    ...    None                 None  None          None   None   \n",
       "2    ...  Kaunas                 None  None          None   None   \n",
       "3    ...    None                 None  None          None   None   \n",
       "4    ...    Lodz     Auschwitz,Dachau  None          None   None   \n",
       "..   ...     ...                  ...   ...           ...    ...   \n",
       "972  ...    None                 None  None          None   None   \n",
       "973  ...    None                 None  None          None   None   \n",
       "974  ...    None                 None  None          None   None   \n",
       "975  ...    None                 None  None          None   None   \n",
       "976  ...    None                 None  None          None   None   \n",
       "\n",
       "    Needs Research Data Entry       Accession  \\\n",
       "0             None         CL     1999.A.0293   \n",
       "1          checked         GG        2003.205   \n",
       "2          checked         GG     1999.A.0024   \n",
       "3             None         CL  1998.A.0119.15   \n",
       "4          checked         GG        2010.249   \n",
       "..             ...        ...             ...   \n",
       "972           None         GG         2004.48   \n",
       "973           None         CL      1990.437.1   \n",
       "974           None         GG            None   \n",
       "975        checked         CL        2004.214   \n",
       "976           None         CL     1999.A.0038   \n",
       "\n",
       "                                                Notes:  Revisit  \n",
       "0                                                 None     None  \n",
       "1                                  Follow-up interview     None  \n",
       "2    This is a follow-up interview to one already d...  checked  \n",
       "3                                                 None     None  \n",
       "4                                                 None     None  \n",
       "..                                                 ...      ...  \n",
       "972                                          Follow-up  checked  \n",
       "973                                               None     None  \n",
       "974                                               None     None  \n",
       "975       Not a survivor, volunteered for the museum?      None  \n",
       "976                                               None     None  \n",
       "\n",
       "[977 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = datasets.load_dataset(\"placingholocaust/testimony-metadata\")[\"train\"]\n",
    "testimonies_metadata = pd.DataFrame(metadata)\n",
    "testimonies_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RG Number</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>USHMM URL</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Middle Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Birth Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>...</th>\n",
       "      <th>Ghetto</th>\n",
       "      <th>Camp(s) Encyclopedia</th>\n",
       "      <th>Camp</th>\n",
       "      <th>Non-SS Camp</th>\n",
       "      <th>Region</th>\n",
       "      <th>Needs Research</th>\n",
       "      <th>Data Entry</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Notes:</th>\n",
       "      <th>Revisit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>RG-50.030.0001</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>David</td>\n",
       "      <td>A.</td>\n",
       "      <td>Kochalski</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1928-05-05</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>West</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          RG Number                                            PDF URL  \\\n",
       "602  RG-50.030.0001  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "\n",
       "                                             USHMM URL First Name Middle Name  \\\n",
       "602  https://collections.ushmm.org/search/catalog/i...      David          A.   \n",
       "\n",
       "     Last Name Birth Name Gender  Birth Date  Birth Year  ... Ghetto  \\\n",
       "602  Kochalski       None      M  1928-05-05      1928.0  ...   None   \n",
       "\n",
       "    Camp(s) Encyclopedia  Camp Non-SS Camp   Region Needs Research Data Entry  \\\n",
       "602                 None  None          None   West           None         CL   \n",
       "\n",
       "    Accession Notes: Revisit  \n",
       "602      None   None    None  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonies_metadata[testimonies_metadata[\"RG Number\"] == \"RG-50.030.0001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RG Number                                                 RG-50.549.02.0033\n",
       "PDF URL                   https://collections.ushmm.org/oh_findingaids/R...\n",
       "USHMM URL                 https://collections.ushmm.org/search/catalog/i...\n",
       "First Name                                                            Hetty\n",
       "Middle Name                                                     d'Ancona de\n",
       "Last Name                                                            Leeuwe\n",
       "Birth Name                                                   Hetty D'Ancona\n",
       "Gender                                                                    F\n",
       "Birth Date                                                       1930-05-01\n",
       "Birth Year                                                           1930.0\n",
       "Place of Birth                                                         None\n",
       "Country                                                                None\n",
       "Experience Group                                                   Survivor\n",
       "Ghetto(s) Encyclopedia                                                 None\n",
       "Ghetto                                                                 None\n",
       "Camp(s) Encyclopedia                                                   None\n",
       "Camp                                                                   None\n",
       "Non-SS Camp                                                            None\n",
       "Region                                                                 None\n",
       "Needs Research                                                         None\n",
       "Data Entry                                                               CL\n",
       "Accession                                                       1999.A.0293\n",
       "Notes:                                                                 None\n",
       "Revisit                                                                None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonies_metadata.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 87381.33it/s]\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/gliner/model.py:568: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_file, map_location=torch.device(map_location))\n",
      "  0%|          | 0/979 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/979 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m\"\u001b[39m\u001b[39mgliner_spacy\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mgliner_model\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mplacingholocaust/gliner_small-v2.1-holocaust\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: labels, \u001b[39m\"\u001b[39m\u001b[39mchunk_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m250\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmap_location\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39m# Usage example:\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m process_files(\u001b[39m\"\u001b[39;49m\u001b[39m../data/03_html_sentences/\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m../data/04_html_ner\u001b[39;49m\u001b[39m\"\u001b[39;49m, nlp, testimonies_metadata)\n",
      "\u001b[1;32m/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     html_content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# Process the HTML content\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m processed_html \u001b[39m=\u001b[39m process_html_with_spacy(html_content, nlp_model, yaml_header)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m# Write the processed content to the output file\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "\u001b[1;32m/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m text \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39mget_text()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Process the text with spaCy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp_model(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Clear the sentence content\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wjbmattingly/projects/data-workflow/examples/04-sents-to-ner.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m sentence\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/gliner_spacy/pipeline.py:74\u001b[0m, in \u001b[0;36mGlinerSpacy.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     70\u001b[0m     chunk_entities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict_entities(chunk, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels,\n\u001b[1;32m     71\u001b[0m                                                  flat_ner\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m                                                  threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold)\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     chunk_entities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_entities(chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels,\n\u001b[1;32m     75\u001b[0m                                                  flat_ner\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m                                                  threshold\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthreshold)\n\u001b[1;32m     78\u001b[0m \u001b[39mfor\u001b[39;00m entity \u001b[39min\u001b[39;00m chunk_entities:\n\u001b[1;32m     79\u001b[0m     all_entities\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     80\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m: offset \u001b[39m+\u001b[39m entity[\u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     81\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m: offset \u001b[39m+\u001b[39m entity[\u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     82\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: entity[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     83\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m: entity[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     84\u001b[0m     })\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/gliner/model.py:205\u001b[0m, in \u001b[0;36mGLiNER.predict_entities\u001b[0;34m(self, text, labels, flat_ner, threshold, multi_label)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_entities\u001b[39m(\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m, text, labels, flat_ner\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, multi_label\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    191\u001b[0m ):\n\u001b[1;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    Predict entities for a single text input.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m        The list of entity predictions.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_predict_entities(\n\u001b[1;32m    206\u001b[0m         [text],\n\u001b[1;32m    207\u001b[0m         labels,\n\u001b[1;32m    208\u001b[0m         flat_ner\u001b[39m=\u001b[39;49mflat_ner,\n\u001b[1;32m    209\u001b[0m         threshold\u001b[39m=\u001b[39;49mthreshold,\n\u001b[1;32m    210\u001b[0m         multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    211\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/gliner/model.py:233\u001b[0m, in \u001b[0;36mGLiNER.batch_predict_entities\u001b[0;34m(self, texts, labels, flat_ner, threshold, multi_label)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39mPredict entities for a batch of texts.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m    The list of lists with predicted entities.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m model_input, raw_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_model_inputs(texts, labels)\n\u001b[0;32m--> 233\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_input)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model_output, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    236\u001b[0m     model_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(model_output)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/gliner/modeling/base.py:160\u001b[0m, in \u001b[0;36mSpanModel.forward\u001b[0;34m(self, input_ids, attention_mask, words_mask, text_lengths, span_idx, span_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m span_rep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_rep_layer(words_embedding, span_idx)\n\u001b[1;32m    158\u001b[0m prompts_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_rep_layer(prompts_embedding)\n\u001b[0;32m--> 160\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mBLKD,BCD->BLKC\u001b[39;49m\u001b[39m\"\u001b[39;49m, span_rep, prompts_embedding)\n\u001b[1;32m    162\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/torch/functional.py:386\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    384\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def create_yaml_header(row):\n",
    "    # Function to create the YAML-like header from a dataframe row\n",
    "    header = \"---\\n\"\n",
    "    header += f\"layout: transcript\\n\"\n",
    "    \n",
    "    # Helper function to safely get values from the row\n",
    "    def safe_get(column, default='none'):\n",
    "        if column in row.index and pd.notna(row[column]):\n",
    "            return str(row[column]).lower()\n",
    "        return default\n",
    "\n",
    "    header += f\"interviewee: {safe_get('First Name')} {safe_get('Middle Name')} {safe_get('Last Name')}\\n\"\n",
    "    header += f\"rg_number: {safe_get('RG Number')}\\n\"\n",
    "    header += f\"pdf_url: {safe_get('PDF URL')}\\n\"\n",
    "    header += f\"ushmm_url: {safe_get('USHMM URL')}\\n\"\n",
    "    header += f\"gender: {safe_get('Gender')}\\n\"\n",
    "    header += f\"birth_date: {safe_get('Birth Date')}\\n\"\n",
    "    header += f\"birth_year: {safe_get('Birth Year')}\\n\"\n",
    "    header += f\"place_of_birth: {safe_get('Place of Birth')}\\n\"\n",
    "    header += f\"country: {safe_get('Country')}\\n\"\n",
    "    header += f\"experience_group: {safe_get('Experience Group')}\\n\"\n",
    "    header += f\"ghetto(s)_encyclopedia: {safe_get('Ghetto(s) Encyclopedia')}\\n\"\n",
    "    header += f\"ghetto: {safe_get('Ghetto')}\\n\"\n",
    "    header += f\"camp(s)_encyclopedia: {safe_get('Camp(s) Encyclopedia')}\\n\"\n",
    "    header += f\"camp: {safe_get('Camp')}\\n\"\n",
    "    header += f\"non_ss_camp: {safe_get('Non-SS Camp')}\\n\"\n",
    "    header += f\"region: {safe_get('Region')}\\n\"\n",
    "    header += f\"needs_research: {safe_get('Needs Research')}\\n\"\n",
    "    header += f\"data_entry: {safe_get('Data Entry')}\\n\"\n",
    "    header += f\"accession: {safe_get('Accession')}\\n\"\n",
    "    header += f\"revisit: {safe_get('Revisit')}\\n\"\n",
    "    header += f\"tags: transcripts\\n\"\n",
    "    header += \"---\\n\\n\"\n",
    "    return header\n",
    "\n",
    "def process_html_with_spacy(html_content, nlp_model, yaml_header):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Add the YAML header to the top of the HTML\n",
    "    header_tag = soup.new_tag(\"pre\")\n",
    "    header_tag.string = yaml_header\n",
    "    soup.insert(0, header_tag)\n",
    "    \n",
    "    # Find all sentence elements\n",
    "    sentences = soup.find_all('sentence')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Get the text content of the sentence\n",
    "        text = sentence.get_text()\n",
    "        \n",
    "        # Process the text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        # Clear the sentence content\n",
    "        sentence.clear()\n",
    "        \n",
    "        # Add annotated content\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            # Add text before the entity\n",
    "            sentence.append(text[last_end:ent.start_char])\n",
    "            \n",
    "            # Create a new span tag\n",
    "            span_tag = soup.new_tag(\"span\", attrs={\"class\": ent.label_})\n",
    "            span_tag.string = text[ent.start_char:ent.end_char]\n",
    "            sentence.append(span_tag)\n",
    "            \n",
    "            last_end = ent.end_char\n",
    "        \n",
    "        # Add any remaining text\n",
    "        sentence.append(text[last_end:])\n",
    "    \n",
    "    # Return the modified HTML as a string\n",
    "    return str(soup)\n",
    "\n",
    "def process_files(input_folder, output_folder, nlp_model, testimonies_data):\n",
    "    # Ensure output folder exists\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    files = os.listdir(input_folder)\n",
    "    files.sort()\n",
    "    # Process each file in the input folder\n",
    "    for filename in tqdm(files):\n",
    "        if filename.endswith('.html'):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Extract the RG Number from the filename\n",
    "            rg_number = filename.split('_')[0]\n",
    "            # print(rg_number)\n",
    "            \n",
    "            # Find the corresponding row in testimonies_data\n",
    "            testimony_row = testimonies_data[testimonies_data['RG Number'] == rg_number]\n",
    "            \n",
    "            if testimony_row.empty:\n",
    "                print(f\"No matching data found for {filename}. Skipping this file.\")\n",
    "                continue\n",
    "            \n",
    "            # Create the YAML header\n",
    "            yaml_header = create_yaml_header(testimony_row.iloc[0])\n",
    "            \n",
    "            # Read the input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Process the HTML content\n",
    "            processed_html = process_html_with_spacy(html_content, nlp_model, yaml_header)\n",
    "            \n",
    "            # Write the processed content to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_html)\n",
    "            \n",
    "            # print(f\"Processed {filename}\")\n",
    "\n",
    "labels = [\"dlf\", \"populated place\", \"country\", \"region\", \"interior space\", \"env feature\", \"building\", \"spatial object\"]\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config={\"gliner_model\": \"placingholocaust/gliner_small-v2.1-holocaust\", \"labels\": labels, \"chunk_size\": 250, \"map_location\": \"mps\"})\n",
    "\n",
    "# Usage example:\n",
    "process_files(\"../data/03_html_sentences/\", \"../data/04_html_ner\", nlp, testimonies_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 33554.43it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed RG-50.030.0001_trs_en_cleaned.html\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_html_with_spacy(html_content, nlp_model):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all sentence elements\n",
    "    sentences = soup.find_all('sentence')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Get the text content of the sentence\n",
    "        text = sentence.get_text()\n",
    "        \n",
    "        # Process the text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        # Clear the sentence content\n",
    "        sentence.clear()\n",
    "        \n",
    "        # Add annotated content\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            # Add text before the entity\n",
    "            sentence.append(text[last_end:ent.start_char])\n",
    "            \n",
    "            # Create a new span tag\n",
    "            span_tag = soup.new_tag(\"span\", attrs={\"class\": ent.label_})\n",
    "            span_tag.string = text[ent.start_char:ent.end_char]\n",
    "            sentence.append(span_tag)\n",
    "            \n",
    "            last_end = ent.end_char\n",
    "        \n",
    "        # Add any remaining text\n",
    "        sentence.append(text[last_end:])\n",
    "    \n",
    "    # Return the modified HTML as a string\n",
    "    return str(soup)\n",
    "\n",
    "def process_files(input_folder, output_folder, nlp_model):\n",
    "    # Ensure output folder exists\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    files = os.listdir(input_folder)\n",
    "    files.sort()\n",
    "    # Process each file in the input folder\n",
    "    for filename in files[:1]:\n",
    "        if filename.endswith('.html'):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Read the input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Process the HTML content\n",
    "            processed_html = process_html_with_spacy(html_content, nlp_model)\n",
    "            \n",
    "            # Write the processed content to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_html)\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "labels = [\"dlf\", \"populated place\", \"country\", \"region\", \"interior space\", \"env feature\", \"building\", \"spatial object\"]\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config={\"gliner_model\": \"placingholocaust/gliner_small-v2.1-holocaust\", \"labels\": labels, \"chunk_size\": 250})\n",
    "\n",
    "# Usage example:\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "process_files(\"../data/03_html_sentences/\", \"../data/04_html_ner\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasthtml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
