{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GLiNER and spaCy for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RG Number</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>USHMM URL</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Middle Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Birth Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>...</th>\n",
       "      <th>Ghetto</th>\n",
       "      <th>Camp(s) Encyclopedia</th>\n",
       "      <th>Camp</th>\n",
       "      <th>Non-SS Camp</th>\n",
       "      <th>Region</th>\n",
       "      <th>Needs Research</th>\n",
       "      <th>Data Entry</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Notes:</th>\n",
       "      <th>Revisit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RG-50.549.02.0033</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Hetty</td>\n",
       "      <td>d'Ancona de</td>\n",
       "      <td>Leeuwe</td>\n",
       "      <td>Hetty D'Ancona</td>\n",
       "      <td>F</td>\n",
       "      <td>1930-05-01</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1999.A.0293</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RG-50.549.02.0072</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Emanuel</td>\n",
       "      <td>None</td>\n",
       "      <td>Mandel</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>GG</td>\n",
       "      <td>2003.205</td>\n",
       "      <td>Follow-up interview</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RG-50.549.02.0035</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Judith</td>\n",
       "      <td>None</td>\n",
       "      <td>Meisel</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Kaunas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>GG</td>\n",
       "      <td>1999.A.0024</td>\n",
       "      <td>This is a follow-up interview to one already d...</td>\n",
       "      <td>checked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RG-50.471.0015</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Esther</td>\n",
       "      <td>None</td>\n",
       "      <td>Lurie</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1998.A.0119.15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RG-50.030.0585</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Eugene</td>\n",
       "      <td>None</td>\n",
       "      <td>Miller</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1923-10-16</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Lodz</td>\n",
       "      <td>Auschwitz,Dachau</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>GG</td>\n",
       "      <td>2010.249</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>RG-50.549.02.0073</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Flory</td>\n",
       "      <td>None</td>\n",
       "      <td>Jagoda</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>1923-12-21</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GG</td>\n",
       "      <td>2004.48</td>\n",
       "      <td>Follow-up</td>\n",
       "      <td>checked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>RG-50.030.0137</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Cornelius</td>\n",
       "      <td>None</td>\n",
       "      <td>Loen</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1922-05-02</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1990.437.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>RG-50.030.0058</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Isaac</td>\n",
       "      <td>None</td>\n",
       "      <td>Danon</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GG</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>RG-50.549.02.0078</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Lucie</td>\n",
       "      <td>None</td>\n",
       "      <td>Rosenberg</td>\n",
       "      <td>None</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>checked</td>\n",
       "      <td>CL</td>\n",
       "      <td>2004.214</td>\n",
       "      <td>Not a survivor, volunteered for the museum?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>RG-50.549.02.0038</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>Isaac</td>\n",
       "      <td>None</td>\n",
       "      <td>Danon</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1929-06-24</td>\n",
       "      <td>1929.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>1999.A.0038</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>977 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             RG Number                                            PDF URL  \\\n",
       "0    RG-50.549.02.0033  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "1    RG-50.549.02.0072  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "2    RG-50.549.02.0035  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "3       RG-50.471.0015  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "4       RG-50.030.0585  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "..                 ...                                                ...   \n",
       "972  RG-50.549.02.0073  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "973     RG-50.030.0137  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "974     RG-50.030.0058  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "975  RG-50.549.02.0078  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "976  RG-50.549.02.0038  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "\n",
       "                                             USHMM URL First Name  \\\n",
       "0    https://collections.ushmm.org/search/catalog/i...      Hetty   \n",
       "1    https://collections.ushmm.org/search/catalog/i...    Emanuel   \n",
       "2    https://collections.ushmm.org/search/catalog/i...     Judith   \n",
       "3    https://collections.ushmm.org/search/catalog/i...     Esther   \n",
       "4    https://collections.ushmm.org/search/catalog/i...     Eugene   \n",
       "..                                                 ...        ...   \n",
       "972  https://collections.ushmm.org/search/catalog/i...      Flory   \n",
       "973  https://collections.ushmm.org/search/catalog/i...  Cornelius   \n",
       "974  https://collections.ushmm.org/search/catalog/i...      Isaac   \n",
       "975  https://collections.ushmm.org/search/catalog/i...      Lucie   \n",
       "976  https://collections.ushmm.org/search/catalog/i...      Isaac   \n",
       "\n",
       "     Middle Name  Last Name      Birth Name Gender  Birth Date  Birth Year  \\\n",
       "0    d'Ancona de     Leeuwe  Hetty D'Ancona      F  1930-05-01      1930.0   \n",
       "1           None     Mandel            None      M        None      1936.0   \n",
       "2           None     Meisel            None      F        None      1929.0   \n",
       "3           None      Lurie            None      F        None         NaN   \n",
       "4           None     Miller            None      M  1923-10-16      1923.0   \n",
       "..           ...        ...             ...    ...         ...         ...   \n",
       "972         None     Jagoda            None      F  1923-12-21      1923.0   \n",
       "973         None       Loen            None      M  1922-05-02      1922.0   \n",
       "974         None      Danon            None      M        None      1929.0   \n",
       "975         None  Rosenberg            None      F        None      1921.0   \n",
       "976         None      Danon            None      M  1929-06-24      1929.0   \n",
       "\n",
       "     ...  Ghetto Camp(s) Encyclopedia  Camp Non-SS Camp   Region  \\\n",
       "0    ...    None                 None  None          None   None   \n",
       "1    ...    None                 None  None          None   None   \n",
       "2    ...  Kaunas                 None  None          None   None   \n",
       "3    ...    None                 None  None          None   None   \n",
       "4    ...    Lodz     Auschwitz,Dachau  None          None   None   \n",
       "..   ...     ...                  ...   ...           ...    ...   \n",
       "972  ...    None                 None  None          None   None   \n",
       "973  ...    None                 None  None          None   None   \n",
       "974  ...    None                 None  None          None   None   \n",
       "975  ...    None                 None  None          None   None   \n",
       "976  ...    None                 None  None          None   None   \n",
       "\n",
       "    Needs Research Data Entry       Accession  \\\n",
       "0             None         CL     1999.A.0293   \n",
       "1          checked         GG        2003.205   \n",
       "2          checked         GG     1999.A.0024   \n",
       "3             None         CL  1998.A.0119.15   \n",
       "4          checked         GG        2010.249   \n",
       "..             ...        ...             ...   \n",
       "972           None         GG         2004.48   \n",
       "973           None         CL      1990.437.1   \n",
       "974           None         GG            None   \n",
       "975        checked         CL        2004.214   \n",
       "976           None         CL     1999.A.0038   \n",
       "\n",
       "                                                Notes:  Revisit  \n",
       "0                                                 None     None  \n",
       "1                                  Follow-up interview     None  \n",
       "2    This is a follow-up interview to one already d...  checked  \n",
       "3                                                 None     None  \n",
       "4                                                 None     None  \n",
       "..                                                 ...      ...  \n",
       "972                                          Follow-up  checked  \n",
       "973                                               None     None  \n",
       "974                                               None     None  \n",
       "975       Not a survivor, volunteered for the museum?      None  \n",
       "976                                               None     None  \n",
       "\n",
       "[977 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = datasets.load_dataset(\"placingholocaust/testimony-metadata\")[\"train\"]\n",
    "testimonies_metadata = pd.DataFrame(metadata)\n",
    "testimonies_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RG Number</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>USHMM URL</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Middle Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Birth Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th>Birth Year</th>\n",
       "      <th>...</th>\n",
       "      <th>Ghetto</th>\n",
       "      <th>Camp(s) Encyclopedia</th>\n",
       "      <th>Camp</th>\n",
       "      <th>Non-SS Camp</th>\n",
       "      <th>Region</th>\n",
       "      <th>Needs Research</th>\n",
       "      <th>Data Entry</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Notes:</th>\n",
       "      <th>Revisit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>RG-50.030.0001</td>\n",
       "      <td>https://collections.ushmm.org/oh_findingaids/R...</td>\n",
       "      <td>https://collections.ushmm.org/search/catalog/i...</td>\n",
       "      <td>David</td>\n",
       "      <td>A.</td>\n",
       "      <td>Kochalski</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1928-05-05</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>West</td>\n",
       "      <td>None</td>\n",
       "      <td>CL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          RG Number                                            PDF URL  \\\n",
       "602  RG-50.030.0001  https://collections.ushmm.org/oh_findingaids/R...   \n",
       "\n",
       "                                             USHMM URL First Name Middle Name  \\\n",
       "602  https://collections.ushmm.org/search/catalog/i...      David          A.   \n",
       "\n",
       "     Last Name Birth Name Gender  Birth Date  Birth Year  ... Ghetto  \\\n",
       "602  Kochalski       None      M  1928-05-05      1928.0  ...   None   \n",
       "\n",
       "    Camp(s) Encyclopedia  Camp Non-SS Camp   Region Needs Research Data Entry  \\\n",
       "602                 None  None          None   West           None         CL   \n",
       "\n",
       "    Accession Notes: Revisit  \n",
       "602      None   None    None  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonies_metadata[testimonies_metadata[\"RG Number\"] == \"RG-50.030.0001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RG Number                                                 RG-50.549.02.0033\n",
       "PDF URL                   https://collections.ushmm.org/oh_findingaids/R...\n",
       "USHMM URL                 https://collections.ushmm.org/search/catalog/i...\n",
       "First Name                                                            Hetty\n",
       "Middle Name                                                     d'Ancona de\n",
       "Last Name                                                            Leeuwe\n",
       "Birth Name                                                   Hetty D'Ancona\n",
       "Gender                                                                    F\n",
       "Birth Date                                                       1930-05-01\n",
       "Birth Year                                                           1930.0\n",
       "Place of Birth                                                         None\n",
       "Country                                                                None\n",
       "Experience Group                                                   Survivor\n",
       "Ghetto(s) Encyclopedia                                                 None\n",
       "Ghetto                                                                 None\n",
       "Camp(s) Encyclopedia                                                   None\n",
       "Camp                                                                   None\n",
       "Non-SS Camp                                                            None\n",
       "Region                                                                 None\n",
       "Needs Research                                                         None\n",
       "Data Entry                                                               CL\n",
       "Accession                                                       1999.A.0293\n",
       "Notes:                                                                 None\n",
       "Revisit                                                                None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonies_metadata.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 71392.41it/s]\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/fasthtml/lib/python3.10/site-packages/gliner/model.py:568: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_file, map_location=torch.device(map_location))\n",
      "  0%|          | 0/979 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def create_yaml_header(row):\n",
    "    # Function to create the YAML-like header from a dataframe row\n",
    "    header = \"---\\n\"\n",
    "    header += f\"layout: transcript\\n\"\n",
    "    \n",
    "    # Helper function to safely get values from the row\n",
    "    def safe_get(column, default='none'):\n",
    "        if column in row.index and pd.notna(row[column]):\n",
    "            return str(row[column]).lower()\n",
    "        return default\n",
    "\n",
    "    header += f\"interviewee: {safe_get('First Name')} {safe_get('Middle Name')} {safe_get('Last Name')}\\n\"\n",
    "    header += f\"rg_number: {safe_get('RG Number')}\\n\"\n",
    "    header += f\"pdf_url: {safe_get('PDF URL')}\\n\"\n",
    "    header += f\"ushmm_url: {safe_get('USHMM URL')}\\n\"\n",
    "    header += f\"gender: {safe_get('Gender')}\\n\"\n",
    "    header += f\"birth_date: {safe_get('Birth Date')}\\n\"\n",
    "    header += f\"birth_year: {safe_get('Birth Year')}\\n\"\n",
    "    header += f\"place_of_birth: {safe_get('Place of Birth')}\\n\"\n",
    "    header += f\"country: {safe_get('Country')}\\n\"\n",
    "    header += f\"experience_group: {safe_get('Experience Group')}\\n\"\n",
    "    header += f\"ghetto(s)_encyclopedia: {safe_get('Ghetto(s) Encyclopedia')}\\n\"\n",
    "    header += f\"ghetto: {safe_get('Ghetto')}\\n\"\n",
    "    header += f\"camp(s)_encyclopedia: {safe_get('Camp(s) Encyclopedia')}\\n\"\n",
    "    header += f\"camp: {safe_get('Camp')}\\n\"\n",
    "    header += f\"non_ss_camp: {safe_get('Non-SS Camp')}\\n\"\n",
    "    header += f\"region: {safe_get('Region')}\\n\"\n",
    "    header += f\"needs_research: {safe_get('Needs Research')}\\n\"\n",
    "    header += f\"data_entry: {safe_get('Data Entry')}\\n\"\n",
    "    header += f\"accession: {safe_get('Accession')}\\n\"\n",
    "    header += f\"revisit: {safe_get('Revisit')}\\n\"\n",
    "    header += f\"tags: transcripts\\n\"\n",
    "    header += \"---\\n\\n\"\n",
    "    return header\n",
    "\n",
    "def process_html_with_spacy(html_content, nlp_model, yaml_header):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Add the YAML header to the top of the HTML\n",
    "    header_tag = soup.new_tag(\"pre\")\n",
    "    header_tag.string = yaml_header\n",
    "    soup.insert(0, header_tag)\n",
    "    \n",
    "    # Find all sentence elements\n",
    "    sentences = soup.find_all('sentence')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Get the text content of the sentence\n",
    "        text = sentence.get_text()\n",
    "        \n",
    "        # Process the text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        # Clear the sentence content\n",
    "        sentence.clear()\n",
    "        \n",
    "        # Add annotated content\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            # Add text before the entity\n",
    "            sentence.append(text[last_end:ent.start_char])\n",
    "            \n",
    "            # Create a new span tag\n",
    "            span_tag = soup.new_tag(\"span\", attrs={\"class\": ent.label_})\n",
    "            span_tag.string = text[ent.start_char:ent.end_char]\n",
    "            sentence.append(span_tag)\n",
    "            \n",
    "            last_end = ent.end_char\n",
    "        \n",
    "        # Add any remaining text\n",
    "        sentence.append(text[last_end:])\n",
    "    \n",
    "    # Return the modified HTML as a string\n",
    "    return str(soup)\n",
    "\n",
    "def process_files(input_folder, output_folder, nlp_model, testimonies_data):\n",
    "    # Ensure output folder exists\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    files = os.listdir(input_folder)\n",
    "    files.sort()\n",
    "    # Process each file in the input folder\n",
    "    for filename in tqdm(files):\n",
    "        if filename.endswith('.html'):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Extract the RG Number from the filename\n",
    "            rg_number = filename.split('_')[0]\n",
    "            # print(rg_number)\n",
    "            \n",
    "            # Find the corresponding row in testimonies_data\n",
    "            testimony_row = testimonies_data[testimonies_data['RG Number'] == rg_number]\n",
    "            \n",
    "            if testimony_row.empty:\n",
    "                print(f\"No matching data found for {filename}. Skipping this file.\")\n",
    "                continue\n",
    "            \n",
    "            # Create the YAML header\n",
    "            yaml_header = create_yaml_header(testimony_row.iloc[0])\n",
    "            \n",
    "            # Read the input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Process the HTML content\n",
    "            processed_html = process_html_with_spacy(html_content, nlp_model, yaml_header)\n",
    "            \n",
    "            # Write the processed content to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_html)\n",
    "            \n",
    "            # print(f\"Processed {filename}\")\n",
    "\n",
    "labels = [\"dlf\", \"populated place\", \"country\", \"region\", \"interior space\", \"env feature\", \"building\", \"spatial object\"]\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config={\"gliner_model\": \"placingholocaust/gliner_small-v2.1-holocaust\", \"labels\": labels, \"chunk_size\": 250, \"map_location\": \"mps\"})\n",
    "\n",
    "# Usage example:\n",
    "process_files(\"../data/03_html_sentences/\", \"../data/04_html_ner\", nlp, testimonies_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 33554.43it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed RG-50.030.0001_trs_en_cleaned.html\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_html_with_spacy(html_content, nlp_model):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all sentence elements\n",
    "    sentences = soup.find_all('sentence')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Get the text content of the sentence\n",
    "        text = sentence.get_text()\n",
    "        \n",
    "        # Process the text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        # Clear the sentence content\n",
    "        sentence.clear()\n",
    "        \n",
    "        # Add annotated content\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            # Add text before the entity\n",
    "            sentence.append(text[last_end:ent.start_char])\n",
    "            \n",
    "            # Create a new span tag\n",
    "            span_tag = soup.new_tag(\"span\", attrs={\"class\": ent.label_})\n",
    "            span_tag.string = text[ent.start_char:ent.end_char]\n",
    "            sentence.append(span_tag)\n",
    "            \n",
    "            last_end = ent.end_char\n",
    "        \n",
    "        # Add any remaining text\n",
    "        sentence.append(text[last_end:])\n",
    "    \n",
    "    # Return the modified HTML as a string\n",
    "    return str(soup)\n",
    "\n",
    "def process_files(input_folder, output_folder, nlp_model):\n",
    "    # Ensure output folder exists\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    files = os.listdir(input_folder)\n",
    "    files.sort()\n",
    "    # Process each file in the input folder\n",
    "    for filename in files[:1]:\n",
    "        if filename.endswith('.html'):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Read the input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "            \n",
    "            # Process the HTML content\n",
    "            processed_html = process_html_with_spacy(html_content, nlp_model)\n",
    "            \n",
    "            # Write the processed content to the output file\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed_html)\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "labels = [\"dlf\", \"populated place\", \"country\", \"region\", \"interior space\", \"env feature\", \"building\", \"spatial object\"]\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config={\"gliner_model\": \"placingholocaust/gliner_small-v2.1-holocaust\", \"labels\": labels, \"chunk_size\": 250})\n",
    "\n",
    "# Usage example:\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "process_files(\"../data/03_html_sentences/\", \"../data/04_html_ner\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasthtml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
